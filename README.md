# Awesome-Video-Multimodal-Large-Language-Models [![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![Awesome MLLM](https://img.shields.io/badge/awesome-video_mllms-blue)](https://github.com/topics/awesome) ![GitHub last commit](https://img.shields.io/github/last-commit/pipixin321/Awesome-Video-MLLMs)

[![SVG Banners](https://svg-banners.vercel.app/api?type=origin&text1=üìπAwesome-Video-MLLM&text2=Video%20üíñ%20Large%20Language%20Model&width=800&height=210)](https://github.com/Akshay090/svg-banners)

üî•üî•üî• Awesome MLLMs/Benchmarks for Short/Long/Streaming Video Understanding

Welcome to stars ‚≠ê & comments üòÄ & sharing :chart_with_upwards_trend: !!
---
### üìñ Contents
- [üöÄ General Works](#general-works)
- [üì∫ Streaming Videos](#streaming-videos)
- [üí° Interesting Works](#interesting-works)
- [üìä Evaluation](#benchmarks-for-evaluation)
---

<!-- |![Star](https://img.shields.io/github/stars/xxx.svg?style=social&label=Star) <br> |-|-| [Github]() | - | -->
#### General Works
|  Title  |   Venue  |   Date   |   Code   |   Frames/FPS   |
|:--------|:--------:|:--------:|:--------:|:--------:|
|![Star](https://img.shields.io/github/stars/OpenGVLab/InternVL.svg?style=social&label=Star) <br> [**InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency**](https://arxiv.org/pdf/2508.18265) | arXiv | 2025-08 | [Github](https://github.com/OpenGVLab/InternVL) | - |
| **MiniCPM-V 4.5: A GPT-4o Level MLLM for Single Image, Multi Image and Video Understanding on Your Phone** | - | 2025-08 | [Github](https://github.com/OpenBMB/MiniCPM-o) | 10fps |
| [**GLM-4.5V**](https://docs.bigmodel.cn/cn/guide/models/vlm/glm-4.5v) | Zhipu AI | 2025-08 | [API](https://open.bigmodel.cn/dev/api#glm-4-5v) | - |
| [**Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities**](https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf) | Google | 2025-06 | - | - |
| [**ERNIE 4.5**](https://yiyan.baidu.com/blog/posts/ernie4.5) | Baidu | 2025-06 | [Github](https://github.com/PaddlePaddle/ERNIE) | - |
| [**Seed1.5-VL Technical Report**](https://arxiv.org/pdf/2505.07062) | arXiv | 2025-05 | - | - |
|![Star](https://img.shields.io/github/stars/quicksviewer/quicksviewer.svg?style=social&label=Star) <br> [**An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes**](https://arxiv.org/pdf/2504.15270) | arXiv | 2025-04 | [Github](https://github.com/quicksviewer/quicksviewer) | - |
|![Star](https://img.shields.io/github/stars/OpenGVLab/InternVL.svg?style=social&label=Star) <br> [**InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models**](https://arxiv.org/abs/2504.10479) | arXiv | 2025-04 | [Github](https://github.com/OpenGVLab/InternVL) | - |
|![Star](https://img.shields.io/github/stars/VITA-MLLM/Long-VITA.svg?style=social&label=Star) <br> [**Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuray**](https://arxiv.org/pdf/2502.05177) | arXiv | 2025-02 | [Github](https://github.com/VITA-MLLM/Long-VITA) | - |
|![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.5-VL.svg?style=social&label=Star) <br> [**Qwen2.5-VL Technical Report**](https://arxiv.org/pdf/2502.13923) | arXiv | 2025-02| [Github](https://github.com/QwenLM/Qwen2.5-VL) | - |
| [**Apollo: An Exploration of Video Understanding in Large Multimodal Models**](https://arxiv.org/pdf/2412.10360)|arXiv|2024-12| - | - |
|![Star](https://img.shields.io/github/stars/TimeMarker-LLM/TimeMarker.svg?style=social&label=Star) <br> [**TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding with Superior Temporal Localization Ability**](https://arxiv.org/pdf/2411.18211)|arXiv|2024-11| [Github](https://github.com/TimeMarker-LLM/TimeMarker) | 128 |
|![Star](https://img.shields.io/github/stars/rhymes-ai/Aria.svg?style=social&label=Star) <br> [**ARIA : An Open Multimodal Native Mixture-of-Experts Model**](https://arxiv.org/pdf/2410.05993) |arXiv|2024-10| [Github](https://github.com/rhymes-ai/Aria) | 256 |
|![Star](https://img.shields.io/github/stars/QwenLM/Qwen2-VL.svg?style=social&label=Star) <br> [**Qwen2-VL: Enhancing Vision-Language Model‚Äôs Perception of the World at Any Resolution**](https://arxiv.org/pdf/2409.12191)|arXiv|2024-10| [Github](https://github.com/QwenLM/Qwen2-VL?tab=readme-ov-file) | 768(2fps) |
|![Star](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&label=Star) <br> [**LLaVA-Video: Video Instruction Tuning With Synthetic Data**](https://arxiv.org/abs/2410.02713)|arXiv|2024-10| [Github](https://llava-vl.github.io/blog/2024-09-30-llava-video/) | 64 |
|![Star](https://img.shields.io/github/stars/Vision-CAIR/LongVU.svg?style=social&label=Star) <br> [**LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding**](https://arxiv.org/abs/2410.17434)|arXiv|2024-10| [Github](https://github.com/Vision-CAIR/LongVU) | 1FPS |
|![Star](https://img.shields.io/github/stars/OpenBMB/MiniCPM-V.svg?style=social&label=Star) <br> [**MiniCPM-V 2.6: A GPT-4V Level MLLM for Single Image, Multi Image and Video on Your Phone**](https://arxiv.org/abs/2408.01800)|arXiv|2023-08| [Github](https://github.com/OpenBMB/MiniCPM-V) | 64 |
|![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star) <br> [**mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models**](https://arxiv.org/abs/2408.04840) |arXiv|2024-08| [Github](https://github.com/X-PLUG/mPLUG-Owl) | 128 |
|![Star](https://img.shields.io/github/stars/OpenGVLab/InternVL.svg?style=social&label=Star) <br> [**InternVL2: Better than the Best‚ÄîExpanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy**](https://internvl.github.io/blog/2024-07-02-InternVL-2.0/)|blog|2024-07| [Github](https://github.com/OpenGVLab/InternVL) | 16 |
|![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA2.svg?style=social&label=Star) <br> [**VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs**](https://arxiv.org/abs/2406.07476) |arXiv|2024-06| [Github](https://github.com/DAMO-NLP-SG/VideoLLaMA2) | 32 |
|![Star](https://img.shields.io/github/stars/ShareGPT4Omni/ShareGPT4Video.svg?style=social&label=Star) <br>  [**ShareGPT4Video: Improving video understanding and generation with better captions**](https://arxiv.org/abs/2406.04325)|arXiv|2024-06| [Github](https://github.com/ShareGPT4Omni/ShareGPT4Video) | 16 |
|![Star](https://img.shields.io/github/stars/EvolvingLMMs-Lab/LongVA.svg?style=social&label=Star) <br> [**LongVA: Long context transfer from language to vision**](https://arxiv.org/abs/2406.16852)|arXiv|2024-06| [Github](https://github.com/EvolvingLMMs-Lab/LongVA) | 1FPS |
|![Star](https://img.shields.io/github/stars/ziplab/LongVLM.svg?style=social&label=Star) <br> [**LongVLM: Efficient long video understanding via large language models**](https://arxiv.org/abs/2404.03384)|ECCV|2024-04| [Github](https://github.com/ziplab/LongVLM) | 100 |
|![Star](https://img.shields.io/github/stars/NVlabs/VILA.svg?style=social&label=Star) <br> [**VILA: On Pre-training for Visual Language Models**](https://arxiv.org/abs/2312.07533)|CVPR|2023-12| [Github](https://github.com/NVlabs/VILA) | 8 |
|![Star](https://img.shields.io/github/stars/RenShuhuai-Andy/TimeChat.svg?style=social&label=Star) <br> [**TimeChat: A time-sensitive multimodal large language model for long video understanding**](https://arxiv.org/abs/2312.02051) |CVPR|2023-12| [Github](https://github.com/RenShuhuai-Andy/TimeChat) | 96 |
|![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Chat-UniVi.svg?style=social&label=Star) <br> [**Chat-UniVi unified visual representation empowers large language models with image and video understanding**](https://arxiv.org/abs/2311.08046)|CVPR|2023-11| [Github](https://github.com/PKU-YuanGroup/Chat-UniVi) | 64 |
|![Star](https://img.shields.io/github/stars/huangb23/VTimeLLM.svg?style=social&label=Star) <br> [**VTimeLLM: Empower LLM to Grasp Video Moments**](https://arxiv.org/abs/2311.18445) <br> |CVPR|2023-11| [Github](https://github.com/huangb23/VTimeLLM) | 100 |
|![Star](https://img.shields.io/github/stars/dvlab-research/LLaMA-VID.svg?style=social&label=Star) <br> [**LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models**](https://arxiv.org/abs/2311.08046v1)|ECCV|2023-11| [Github](https://github.com/dvlab-research/LLaMA-VID) | 1FPS |
|![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Video-LLaVA.svg?style=social&label=Star) <br> [**Video-LLaVA: Learning united visual representation by alignment before projection**](https://arxiv.org/abs/2311.10122)|arXiv|2023-11| [Github](https://github.com/PKU-YuanGroup/Video-LLaVA) | 8 |
|![Star](https://img.shields.io/github/stars/rese1f/MovieChat.svg?style=social&label=Star) <br> [**MovieChat: From Dense Token to Sparse Memory for Long Video Understanding**](https://arxiv.org/abs/2307.16449v4)|arXiv|2023-07| [Github](https://github.com/rese1f/MovieChat) | 2048 |
|![Star](https://img.shields.io/github/stars/mbzuai-oryx/Video-ChatGPT.svg?style=social&label=Star) <br> [**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**](https://arxiv.org/abs/2306.05424)|ACL|2023-06| [Github](https://github.com/mbzuai-oryx/Video-ChatGPT) | 100 |
|![Star](https://img.shields.io/github/stars/RupertLuo/Valley.svg?style=social&label=Star) <br> [**VALLEY: Video Assistant with Large Language model Enhanced ability**](https://arxiv.org/abs/2306.07207)|arXiv|2023-06| [Github](https://github.com/RupertLuo/Valley) | 0.5FPS |
|![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA.svg?style=social&label=Star) <br> [**Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding**](https://arxiv.org/abs/2306.02858)|EMNLP|2023-06| [Github](https://github.com/DAMO-NLP-SG/Video-LLaMA) | 8 |
|![Star](https://img.shields.io/github/stars/OpenGVLab/Ask-Anything.svg?style=social&label=Star) <br> [**VideoChat: Chat-Centric Video Understanding**](httpshttps://arxiv.org/abs/2305.06355) |arXiv|2023-05| [Github](https://github.com/OpenGVLab/Ask-Anything) | 4~32 |
|![Star](https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter.svg?style=social&label=Star) <br> [**LLaMA-Adapter: Efficient Fine-tuning of LLaMA**](https://arxiv.org/pdf/2303.16199) |ICLR|2023-03| [Github](https://github.com/OpenGVLab/LLaMA-Adapter) | - |

#### Streaming Videos
|  Title  |   Venue  |   Date   |   Code   |   Frames   |
|:--------|:--------:|:--------:|:--------:|:--------:|
|![Star](https://img.shields.io/github/stars/JoeLeelyf/OVO-Bench.svg?style=social&label=Star) <br>[**[Benchmark] OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?**](https://arxiv.org/pdf/2501.05510) |arXiv|2025-01| [Github](https://github.com/JoeLeelyf/OVO-Bench) | Streaming |
|![Star](https://img.shields.io/github/stars/Mark12Ding/Dispider.svg?style=social&label=Star) <br>[**Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction**](https://arxiv.org/pdf/2501.03218) |arXiv|2025-01| [Github](https://github.com/Mark12Ding/Dispider) | Streaming |
|![Star](https://img.shields.io/github/stars/VITA-MLLM/VITA.svg?style=social&label=Star) <br>[**VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction**](https://arxiv.org/pdf/2501.01957) |arXiv|2025-01| [Github](https://github.com/VITA-MLLM/VITA) | Streaming |
| [**Streaming long video understanding with large language models**](https://www.arxiv.org/abs/2405.16009)|arXiv|2024-05| - | 16(Streaming) |


#### Interesting Works
|  Title  |   Venue  |   Date   |   Code   |   Frames   |
|:--------|:--------:|:--------:|:--------:|:--------:|
|![Star](https://img.shields.io/github/stars/VITA-MLLM/Sparrow.svg?style=social&label=Star) <br> [**Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation**](https://arxiv.org/pdf/2411.19951) | arXiv | 2025-03 | [Github](https://github.com/VITA-MLLM/Sparrow) | - |
|![Star](https://img.shields.io/github/stars/xjtupanda/T2Vid.svg?style=social&label=Star) <br> [**T2Vid: Translating Long Text into Multi-Image is the Catalyst for Video-LLMs**](https://arxiv.org/pdf/2411.19951)|arXiv|2024-12| [Github](https://github.com/xjtupanda/T2Vid) | - |



### Benchmarks for Evaluation
#### General
##### [:bar_chart: Opencomprass Leaderboard](https://huggingface.co/spaces/opencompass/openvlm_video_leaderboard)
<!-- |![Star](https://img.shields.io/github/stars/xxx.svg?style=social&label=Star) <br> |-|-| [Github]() | - | -->
|  Title  |   Venue  |   Date   |   Repo   | LeaderBoard|
|:--------|:--------:|:--------:|:--------:|:--------:|
|![Star](https://img.shields.io/github/stars/OpenGVLab/Ask-Anything.svg?style=social&label=Star) <br> [**MVBench: A Comprehensive Multi-modal Video Understanding Benchmark**](https://arxiv.org/abs/2311.17005)|-|2023-12| [Github](https://github.com/OpenGVLab/Ask-Anything) |-|
|![Star](https://img.shields.io/github/stars/llyx97/TempCompass.svg?style=social&label=Star) <br> [**TempCompass: Do Video LLMs Really Understand Videos?**](https://arxiv.org/abs/2403.00476)|ACL|2024-03| [Github](https://github.com/llyx97/TempCompass) |-|
|![Star](https://img.shields.io/github/stars/BradyFU/Video-MME.svg?style=social&label=Star) <br> [**Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis**](https://arxiv.org/abs/2405.21075)|-|2024-06| [Github](https://github.com/BradyFU/Video-MME) |-|
|![Star](https://img.shields.io/github/stars/open-compass/VLMEvalKit.svg?style=social&label=Star) <br> [**MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding**](https://arxiv.org/abs/2406.14515)|NIPS D&B|2024-06| [Github](https://github.com/open-compass/VLMEvalKit) |-|
|![Star](https://img.shields.io/github/stars/JUNJIE99/MLVU.svg?style=social&label=Star) <br> [**MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding**](https://arxiv.org/abs/2406.04264)|arXiv|2024-06| [Github](https://github.com/JUNJIE99/MLVU) |-|
|![Star](https://img.shields.io/github/stars/xxx.svg?style=social&label=Star) <br> [**HourVideo: 1-Hour Video-Language Understanding**](https://arxiv.org/abs/2411.04998) |NIPS D&B|2024-11| [Github](https://github.com/keshik6/HourVideo) | [comming soon](https://github.com/keshik6/HourVideo)|