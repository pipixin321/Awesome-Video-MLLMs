# Awesome-Video-Multimodal-Large-Language-Models [![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![Awesome MLLM](https://img.shields.io/badge/awesome-video_mllms-blue)](https://github.com/topics/awesome)

:fire: :fire: :fire: Awesome MLLMs/Benchmarks for Short/Long/Streaming Video Understanding :video_camera:

Welcome to stars ‚≠ê & comments üíπ & sharing üòÄ !!
---
### Contents
- [Methods](#awesome-methods)
- [Evaluation](#awesome-benchmarks)
---


### Awesome Methods
|  Title  |   Venue  |   Date   |   Code   |   Frame   |
|:--------|:--------:|:--------:|:--------:|:--------:|
|![Star](https://img.shields.io/github/stars/xxx.svg?style=social&label=Star) <br> |-|-| [Github]() | - |
|![Star](https://img.shields.io/github/stars/dvlab-research/LLaMA-VID.svg?style=social&label=Star) <br> [**LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models**](https://arxiv.org/abs/2311.08046v1)|ECCV|2023-11| [Github](https://github.com/dvlab-research/LLaMA-VID) | 1FPS |
|![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Video-LLaVA.svg?style=social&label=Star) <br> [**Video-LLaVA: learning united visual representation by alignment before projection**](https://arxiv.org/abs/2311.10122)|arXiv|2023-11| [Github](https://github.com/PKU-YuanGroup/Video-LLaVA) | 8 |
|![Star](https://img.shields.io/github/stars/rese1f/MovieChat.svg?style=social&label=Star) <br> [**MovieChat: From Dense Token to Sparse Memory for Long Video Understanding**](https://arxiv.org/abs/2307.16449v4)|arXiv|2023-07| [Github](https://github.com/rese1f/MovieChat) | 2048 |
|![Star](https://img.shields.io/github/stars/mbzuai-oryx/Video-ChatGPT.svg?style=social&label=Star) <br> [**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**](https://arxiv.org/abs/2306.05424)|ACL|2023-06| [Github](https://github.com/mbzuai-oryx/Video-ChatGPT) | 100 |
|![Star](https://img.shields.io/github/stars/RupertLuo/Valley.svg?style=social&label=Star) <br> [**VALLEY: Video Assistant with Large Language model Enhanced ability**](https://arxiv.org/abs/2306.07207)|arXiv|2023-06| [Github](https://github.com/RupertLuo/Valley) | 0.5FPS |
|![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA.svg?style=social&label=Star) <br> [**Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding**](https://arxiv.org/abs/2306.02858)|EMNLP|2023-06| [Github](https://github.com/DAMO-NLP-SG/Video-LLaMA) | 8 |
|![Star](https://img.shields.io/github/stars/OpenGVLab/Ask-Anything.svg?style=social&label=Star) <br> [**VideoChat: Chat-Centric Video Understanding**](https://arxiv.org/abs/2305.06355) |arXiv|2023-05| [Github](https://github.com/OpenGVLab/Ask-Anything) | 4~32 |
|![Star](https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter.svg?style=social&label=Star) <br> [**LLaMA-Adapter: Efficient Fine-tuning of LLaMA**](https://arxiv.org/pdf/2303.16199) |ICLR|2023-03| [Github](https://github.com/OpenGVLab/LLaMA-Adapter) | - |

### Awesome Benchmarks