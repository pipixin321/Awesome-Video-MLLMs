# Awesome-Video-Multimodal-Large-Language-Models [![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![Awesome MLLM](https://img.shields.io/badge/awesome-video_mllms-blue)](https://github.com/topics/awesome)

:fire: :fire: :fire: Awesome MLLMs/Benchmarks for Short/Long/Streaming Video Understanding :video_camera:

Welcome to stars ‚≠ê & comments üíπ & sharing üòÄ !!
---
### Contents
- [Methods](#awesome-methods)
- [Evaluation](#awesome-benchmarks)
---

<!-- |![Star](https://img.shields.io/github/stars/xxx.svg?style=social&label=Star) <br> |-|-| [Github]() | - | -->
### Awesome MLLMs
|  Title  |   Venue  |   Date   |   Code   |   Frames   |
|:--------|:--------:|:--------:|:--------:|:--------:|
|![Star](https://img.shields.io/github/stars/QwenLM/Qwen2-VL.svg?style=social&label=Star) <br> [**Qwen2-VL: Enhancing Vision-Language Model‚Äôs Perception of the World at Any Resolution**](https://arxiv.org/pdf/2409.12191)|arXiv|2024-10| [Github](https://github.com/QwenLM/Qwen2-VL?tab=readme-ov-file) | 768(2fps) |
|![Star](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&label=Star) <br> [**LLaVA-Video: Video Instruction Tuning With Synthetic Data**](https://arxiv.org/abs/2410.02713)|arXiv|2024-10| [Github](https://llava-vl.github.io/blog/2024-09-30-llava-video/) | 64 |
|![Star](https://img.shields.io/github/stars/Vision-CAIR/LongVU.svg?style=social&label=Star) <br> [**LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding**](https://arxiv.org/abs/2410.17434)|arXiv|2024-10| [Github](https://github.com/Vision-CAIR/LongVU) | 1FPS |
|![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star) <br> [**mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models**](https://arxiv.org/abs/2408.04840) |arXiv|2024-08| [Github](https://github.com/X-PLUG/mPLUG-Owl) | 128 |
|![Star](https://img.shields.io/github/stars/OpenGVLab/InternVL.svg?style=social&label=Star) <br> [**InternVL2: Better than the Best‚ÄîExpanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy**](https://internvl.github.io/blog/2024-07-02-InternVL-2.0/)|blog|2024-07| [Github](https://github.com/OpenGVLab/InternVL) | 16 |
|![Star](https://img.shields.io/github/stars/EvolvingLMMs-Lab/LongVA.svg?style=social&label=Star) <br> [**LongVA: Long context transfer from language to vision**](https://arxiv.org/abs/2406.16852)|arXiv|2024-06| [Github](https://github.com/EvolvingLMMs-Lab/LongVA) | 1FPS |
|![Star](https://img.shields.io/github/stars/ziplab/LongVLM.svg?style=social&label=Star) <br> [**LongVLM: Efficient long video understanding via large language models**](https://arxiv.org/abs/2404.03384)|ECCV|2024-04| [Github](https://github.com/ziplab/LongVLM) | 100 |
|![Star](https://img.shields.io/github/stars/NVlabs/VILA.svg?style=social&label=Star) <br> [**VILA: On Pre-training for Visual Language Models**](https://arxiv.org/abs/2312.07533)|CVPR|2023-12| [Github](https://github.com/NVlabs/VILA) | 8 |
|![Star](https://img.shields.io/github/stars/RenShuhuai-Andy/TimeChat.svg?style=social&label=Star) <br> [**TimeChat: A time-sensitive multimodal large language model for long video understanding**](https://arxiv.org/abs/2312.02051) |CVPR|2023-12| [Github](https://github.com/RenShuhuai-Andy/TimeChat) | 96 |
|![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Chat-UniVi.svg?style=social&label=Star) <br> [**Chat-UniVi unified visual representation empowers large language models with image and video understanding**](https://arxiv.org/abs/2311.08046)|CVPR|2023-11| [Github](https://github.com/PKU-YuanGroup/Chat-UniVi) | 64 |
|![Star](https://img.shields.io/github/stars/huangb23/VTimeLLM.svg?style=social&label=Star) <br> [**VTimeLLM: Empower LLM to Grasp Video Moments**](https://arxiv.org/abs/2311.18445) <br> |CVPR|2023-11| [Github](https://github.com/huangb23/VTimeLLM) | 100 |
|![Star](https://img.shields.io/github/stars/dvlab-research/LLaMA-VID.svg?style=social&label=Star) <br> [**LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models**](https://arxiv.org/abs/2311.08046v1)|ECCV|2023-11| [Github](https://github.com/dvlab-research/LLaMA-VID) | 1FPS |
|![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Video-LLaVA.svg?style=social&label=Star) <br> [**Video-LLaVA: Learning united visual representation by alignment before projection**](https://arxiv.org/abs/2311.10122)|arXiv|2023-11| [Github](https://github.com/PKU-YuanGroup/Video-LLaVA) | 8 |
|![Star](https://img.shields.io/github/stars/rese1f/MovieChat.svg?style=social&label=Star) <br> [**MovieChat: From Dense Token to Sparse Memory for Long Video Understanding**](https://arxiv.org/abs/2307.16449v4)|arXiv|2023-07| [Github](https://github.com/rese1f/MovieChat) | 2048 |
|![Star](https://img.shields.io/github/stars/mbzuai-oryx/Video-ChatGPT.svg?style=social&label=Star) <br> [**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**](https://arxiv.org/abs/2306.05424)|ACL|2023-06| [Github](https://github.com/mbzuai-oryx/Video-ChatGPT) | 100 |
|![Star](https://img.shields.io/github/stars/RupertLuo/Valley.svg?style=social&label=Star) <br> [**VALLEY: Video Assistant with Large Language model Enhanced ability**](https://arxiv.org/abs/2306.07207)|arXiv|2023-06| [Github](https://github.com/RupertLuo/Valley) | 0.5FPS |
|![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA.svg?style=social&label=Star) <br> [**Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding**](https://arxiv.org/abs/2306.02858)|EMNLP|2023-06| [Github](https://github.com/DAMO-NLP-SG/Video-LLaMA) | 8 |
|![Star](https://img.shields.io/github/stars/OpenGVLab/Ask-Anything.svg?style=social&label=Star) <br> [**VideoChat: Chat-Centric Video Understanding**](https://arxiv.org/abs/2305.06355) |arXiv|2023-05| [Github](https://github.com/OpenGVLab/Ask-Anything) | 4~32 |
|![Star](https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter.svg?style=social&label=Star) <br> [**LLaMA-Adapter: Efficient Fine-tuning of LLaMA**](https://arxiv.org/pdf/2303.16199) |ICLR|2023-03| [Github](https://github.com/OpenGVLab/LLaMA-Adapter) | - |

### Awesome Benchmarks for Evaluation
#### [:bar_chart: Opencomprass Leaderboard](https://huggingface.co/spaces/opencompass/openvlm_video_leaderboard)
<!-- |![Star](https://img.shields.io/github/stars/xxx.svg?style=social&label=Star) <br> |-|-| [Github]() | -->
|  Title  |   Venue  |   Date   |   Repo   | LeaderBoard|
|:--------|:--------:|:--------:|:--------:|:--------:|
|![Star](https://img.shields.io/github/stars/OpenGVLab/Ask-Anything.svg?style=social&label=Star) <br> [**MVBench: A Comprehensive Multi-modal Video Understanding Benchmark**](https://arxiv.org/abs/2311.17005)|-|2023-12| [Github](https://github.com/OpenGVLab/Ask-Anything) |-|
|![Star](https://img.shields.io/github/stars/llyx97/TempCompass.svg?style=social&label=Star) <br> [**TempCompass: Do Video LLMs Really Understand Videos?**](https://arxiv.org/abs/2403.00476)|ACL|2024-03| [Github](https://github.com/llyx97/TempCompass) |-|
|![Star](https://img.shields.io/github/stars/BradyFU/Video-MME.svg?style=social&label=Star) <br> [**Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis**](https://arxiv.org/abs/2405.21075)|-|2024-06| [Github](https://github.com/BradyFU/Video-MME) |-|
|![Star](https://img.shields.io/github/stars/open-compass/VLMEvalKit.svg?style=social&label=Star) <br> [**MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding**](https://arxiv.org/abs/2406.14515)|NIPS D&B|2024-06| [Github](https://github.com/open-compass/VLMEvalKit) |-|
|![Star](https://img.shields.io/github/stars/JUNJIE99/MLVU.svg?style=social&label=Star) <br> [**MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding**](https://arxiv.org/abs/2406.04264)|arXiv|2024-06| [Github](https://github.com/JUNJIE99/MLVU) |-|
|![Star](https://img.shields.io/github/stars/xxx.svg?style=social&label=Star) <br> [**HourVideo: 1-Hour Video-Language Understanding**](https://arxiv.org/abs/2411.04998) |NIPS D&B|2024-11| [Github](https://github.com/keshik6/HourVideo) | [comming soon](https://github.com/keshik6/HourVideo)|
